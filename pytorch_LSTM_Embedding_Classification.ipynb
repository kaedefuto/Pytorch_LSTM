{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊PYTORCHによるLSTMの実装\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 環境:Ubuntu18.04\n",
    "- GPU:Quadro RTX 8000\n",
    "- ドライバー:NVIDIA-SMI 460.32.03, Driver Version: 460.32.03, CUDA Version: 11.2\n",
    "- ./data:train.tsv, test.tsv,test.csv\n",
    "- EarlyStoppingを利用する場合はhttps://github.com/Bjarten/early-stopping-pytorch からpytorchtools.pyをutilsにインストールし学習・検証のコメントアウトを外すこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊事前準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n pytorch_bert python=3.7\n",
    "!conda activate pytorch_bert\n",
    "!conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge\n",
    "!conda install jupyter\n",
    "!pip install torchtext==0.9.0\n",
    "!pip install tqdm\n",
    "!pip install attrdict\n",
    "!pip install mojimoji\n",
    "!pip install mecab-python3\n",
    "!pip install mecabrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊ライブラリの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import glob\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import mojimoji\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_result=\"./result/\"\n",
    "path_weights=\"./weights/\"\n",
    "max_length=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):\n",
    "    # 半角・全角の統一（半角から全角へ変換）\n",
    "    text = mojimoji.han_to_zen(text) \n",
    "    # 改行、半角スペース、全角スペースを削除\n",
    "    text = re.sub('\\r', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('　', '', text)\n",
    "    text = re.sub(' ', '', text)\n",
    "    #どっちでも\n",
    "    #text = re.sub(',', '', text)\n",
    "\n",
    "    # 数字文字の一律「0」化\n",
    "    #text = re.sub(r'[0-9 ０-９]+', '0', text)  # 数字\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        #if (p == \".\"):\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\"\"\"\n",
    "    *初期\n",
    "    mecabrc:(デフォルト)\n",
    "    -Ochasen:(ChaSen 互換形式)\n",
    "    -Owakati:(分かち書きのみを出力)\n",
    "    -Oyomi:(読みのみを出力)\n",
    "\n",
    "    *自分の環境の辞書も使える\n",
    "    -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd:neologd辞書\n",
    "    \"\"\"\n",
    "def mecab_tokenizer(text):\n",
    "    tagger = MeCab.Tagger (\"/etc/mecabrc\")\n",
    "    #tagger = MeCab.Tagger(\"-Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n",
    "    #tagger = MeCab.Tagger (\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    #node = tagger.parse(text)\n",
    "    #print(node.split(' '))\n",
    "    #print(tagger.parse(text).split())\n",
    "    return tagger.parse(text).split()#\" \".join(tagger.parse(text).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊Datasetの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数のシードを設定\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = mecab_tokenizer(text)\n",
    "    #\"\"\"\n",
    "    #train_x_vec=[]\n",
    "    #words_list_train = sentence2words(ret) # str\"word word ... word\" → list[word, word, ... , word]\n",
    "    #train_x_vec = To_vec(words_list_train, xp) # list[word, word, ... , word] → np.array[[vector], [vector], ... , [vector]]\n",
    "    #print(train_x_vec)\n",
    "    #\"\"\"\n",
    "    return ret\n",
    "\n",
    "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True, lower=True, include_lengths=True, batch_first=True, fix_length=max_length, unk_token='<unk>', pad_token='<pad>')\n",
    "#init_token=\"<cls>\", eos_token=\",eos>\"\n",
    "\n",
    "LABEL = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# フォルダ「data」から各tsvファイルを読み込みます\n",
    "train_val_ds, test_ds = torchtext.legacy.data.TabularDataset.splits(\n",
    "    path=\"data/\", train='train.tsv',\n",
    "    test='test.tsv', format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊単語の分散表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "w2v_vectors = Vectors(name='../NWJC2VEC/nwjc_word_skip_300_8_25_0_1e4_6_1_0_15.txt.vec')\n",
    "#fasttext = torchtext.vocab.FastText(language=\"en\")\n",
    "\n",
    "print(\"1単語を表現する次元数：\", w2v_vectors.dim)\n",
    "print(\"単語数：\",len(w2v_vectors.itos))\n",
    "\n",
    "#print(len(w2v_vectors.itos)) \n",
    "#print(w2v_vectors.vectors[w2v_vectors.stoi[\"台風\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊Dataloaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "# torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
    "train_ds, val_ds = train_val_ds.split(split_ratio=0.8, random_state=random.seed(1234))\n",
    "\n",
    "TEXT.build_vocab(train_val_ds, test_ds, vectors=w2v_vectors, min_freq=1)\n",
    "#print(TEXT.vocab.freqs) # 単語毎の出現回数\n",
    "#print(TEXT.vocab.stoi) # 文字列からインデックス番号\n",
    "#print(TEXT.vocab.itos) # インデックス番号から文字列\n",
    "#print(TEXT.vocab.vectors) # 単語ベクトル\n",
    "#print(TEXT.vocab.vectors.size()) # 単語ベクトルのサイズ\n",
    "\n",
    "#print(text_field.vocab.vectors.shape)  # torch.Size([xxx, 300])\n",
    "#print(text_field.vocab.itos[:10])\n",
    "#print(TEXT.vocab.stoi[\"台風\"]) \n",
    "#print(text_field.vocab.itos[7]) \n",
    "#print(TEXT.vocab.vectors[TEXT.vocab.stoi[\"台風\"]])\n",
    "# 存在しないトークン -> \"<unk>\"\n",
    "#print(text_field.vocab.stoi[\"is\"])  # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 32  # BERTでは16、32あたりを使用する\n",
    "train_dl = torchtext.legacy.data.Iterator(train_val_ds, batch_size=batch_size, train=True)\n",
    "train_dl_val = torchtext.legacy.data.Iterator(train_ds, batch_size=batch_size, train=True)\n",
    "val_dl = torchtext.legacy.data.Iterator(val_ds, batch_size=batch_size, train=False, sort=False)\n",
    "test_dl = torchtext.legacy.data.Iterator(test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict_val = {\"train\": train_dl_val, \"val\": val_dl}\n",
    "\n",
    "dataloaders_dict = {\"train\": train_dl}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認 テストデータのデータセットで確認\n",
    "batch = next(iter(test_dl))\n",
    "print(batch)\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊LSTMモデルを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, vocab_size, w2v_vectors):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        \n",
    "        # embedding\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings=w2v_vectors, freeze=True, padding_idx=0)\n",
    "        #self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        #self.embedding.weight.data.copy_(w2v_vectors)\n",
    "        \n",
    "        # BILSTM：前方向と後ろ方向の最後の隠れ層ベクトルを結合したものを受け取るので、hidden_dimを2倍している\n",
    "        self.bilstm = nn.LSTM(input_size=input_size,  hidden_size=hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.cls = nn.Linear(in_features=hidden_size * 2, out_features=2)\n",
    "        \n",
    "        # LSTM\n",
    "        #self.lstm = nn.LSTM(input_size=input_size,  hidden_size=hidden_size, batch_first=True) #多層：num_layers = 2\n",
    "        #self.cls = nn.Linear(in_features=hidden_size, out_features=2)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        encoded_layers=self.embedding(input_ids)\n",
    "        #print(encoded_layers.shape)\n",
    "        \n",
    "        #BILSTM, LSTM\n",
    "        # output：各単語に対する出力値の列[32, 128, 300]\n",
    "        # (hn, cn)：LSTMブロックの最後の出力（隠れ層, メモリセル）[32, 1, 300]\n",
    "        #output, (hn, cn) = self.bilstm(encoded_layers)\n",
    "        \n",
    "        #BILSTM\n",
    "        output, hn =self.bilstm(encoded_layers) # 隠れ層の最後\n",
    "        output_hn = torch.cat([hn[0][0], hn[0][1]], dim=1) #隠れ層の結合\n",
    "        \n",
    "        #LSTM\n",
    "        #output, (hn, cn) = self.lstm(encoded_layers)\n",
    "        #output_hn = hn\n",
    "        \n",
    "        out = self.cls(output_hn)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル構築\n",
    "net = LSTMClassifier(input_size = 300, hidden_size = 300, vocab_size = TEXT.vocab.vectors.size(0), w2v_vectors = TEXT.vocab.vectors)\n",
    "# 訓練モードに設定\n",
    "net.train()\n",
    "\n",
    "print('ネットワーク設定完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊LSTMのファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "\n",
    "optimizer = optim.Adam(net.bilstm.parameters(), lr=1e-3)\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊学習・検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊ 開発データでハイパーパラメータを決定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "#from utils.pytorchtools import EarlyStopping\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    #エポック数,Acuraccy,Loss保存用\n",
    "    Epochs=[]\n",
    "    Accuracy_train=[]\n",
    "    Loss_train=[]\n",
    "    Accuracy_val=[]\n",
    "    Loss_val=[]\n",
    "    \n",
    "    #イテレータのエポック数,Acuraccy,Loss保存用\n",
    "    Epochs_it=[]\n",
    "    Accuracy_train_it=[]\n",
    "    Loss_train_it=[]\n",
    "    \n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "    #early_stopping = EarlyStopping(patience=10)\n",
    "    \n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "    n=0\n",
    "    \n",
    "    #時間\n",
    "    start =time.time()\n",
    "    \n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        \"\"\"\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early Stopping\")\n",
    "            break # 打ち切り\n",
    "        # epochごとの訓練と検証のループ\n",
    "        \"\"\"\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            count=0\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "                count=1\n",
    "            \n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # 開始時刻を保存\n",
    "            t_epoch_start = time.time()\n",
    "            t_iter_start = time.time()\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            #print(dataloaders_dict[phase])\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.Text[0].to(device)  # 文章\n",
    "                labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # 入力\n",
    "                    outputs = net(inputs)\n",
    "\n",
    "                    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "                    \n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        \n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            acc = (torch.sum(preds == labels.data)).double()/batch_size\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec. || 本イテレーションの正解率：{}'.format(iteration, loss.item(), duration, acc))\n",
    "                            t_iter_start = time.time()\n",
    "                            \n",
    "                            \"\"\"\n",
    "                            Epochs_it.append(iteration/10)\n",
    "                            Accuracy_train_it.append(acc)\n",
    "                            Loss_train_it.append(loss.item())\n",
    "                            \"\"\"\n",
    "                            #early_stopping(loss.item(), net)\n",
    "                                       \n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "                    \n",
    "            # epochごとのlossと正解率\n",
    "            t_epoch_finish = time.time()\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double(\n",
    "            ) / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            a=float(\"{:.4f}\".format(epoch_acc))\n",
    "                \n",
    "            t_epoch_start = time.time()\n",
    "            \n",
    "            if count == 0:\n",
    "                Loss_train.append(epoch_loss)\n",
    "                Accuracy_train.append(a)\n",
    "            elif count==1:\n",
    "                Loss_val.append(epoch_loss)\n",
    "                Accuracy_val.append(a)\n",
    "        Epochs.append(epoch+1)\n",
    "    t=time.time()\n",
    "    print(\"Time:{:.4f}sec\".format(t-start))\n",
    "        \n",
    "    return net,Epochs,Loss_train,Accuracy_train,Loss_val,Accuracy_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 学習・検証を実行する。1epochに20分ほどかかります\n",
    "num_epochs = 10\n",
    "net_trained, Epochs, Loss_train, Accuracy_train, Loss_val, Accuracy_val = train_model(net, dataloaders_dict_val,\n",
    "                          criterion, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊学習時のEpochsごとのAccuracyを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#print(Accuracy_train)\n",
    "#print(Accuracy_val)\n",
    "#Accuracy_test=[]\n",
    "plt.plot(Epochs,Accuracy_train,color=\"blue\",label=\"train\")\n",
    "plt.plot(Epochs,Accuracy_val,color=\"red\",label=\"val\")\n",
    "#plt.plot(Epochs,Accuracy_test,color=\"red\",label=\"test\")\n",
    "plt.xticks(Epochs)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig(path_result+\"acc_val.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊学習時のEpochsごとのLossを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.axes().set_aspect(\"equal\")\n",
    "#Loss_test=[]\n",
    "plt.plot(Epochs, Loss_train,color=\"blue\",label=\"train\")\n",
    "plt.plot(Epochs, Loss_val,color=\"red\",label=\"val\")\n",
    "#plt.plot(Epochs,Loss_test,color=\"red\",label=\"test\")\n",
    "plt.xticks(Epochs)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig(path_result+\"Loss_val.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊ 全データで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "#from utils.pytorchtools import EarlyStopping\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    #エポック数,Acuraccy,Loss保存用\n",
    "    Epochs=[]\n",
    "    Accuracy_train=[]\n",
    "    Loss_train=[]\n",
    "    Accuracy_val=[]\n",
    "    Loss_val=[]\n",
    "    \n",
    "    #イテレータのエポック数,Acuraccy,Loss保存用\n",
    "    Epochs_it=[]\n",
    "    Accuracy_train_it=[]\n",
    "    Loss_train_it=[]\n",
    "    \n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "    #early_stopping = EarlyStopping(patience=10)\n",
    "    \n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "    n=0\n",
    "    \n",
    "    #時間\n",
    "    start =time.time()\n",
    "    \n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        \"\"\"\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early Stopping\")\n",
    "            break # 打ち切り\n",
    "        # epochごとの訓練と検証のループ\n",
    "        \"\"\"\n",
    "        \n",
    "        #for phase in ['train', 'val']:\n",
    "        for phase in ['train']:\n",
    "            count=0\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "                count=1\n",
    "            \n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # 開始時刻を保存\n",
    "            t_epoch_start = time.time()\n",
    "            t_iter_start = time.time()\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.Text[0].to(device)  # 文章\n",
    "                labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # 入力\n",
    "                    outputs = net(inputs)\n",
    "\n",
    "                    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "                    \n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        \n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            acc = (torch.sum(preds == labels.data)).double()/batch_size\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec. || 本イテレーションの正解率：{}'.format(iteration, loss.item(), duration, acc))\n",
    "                            t_iter_start = time.time()\n",
    "                            \n",
    "                            \"\"\"\n",
    "                            Epochs_it.append(iteration/10)\n",
    "                            Accuracy_train_it.append(acc)\n",
    "                            Loss_train_it.append(loss.item())\n",
    "                            \"\"\"\n",
    "                            #early_stopping(loss.item(), net)\n",
    "                                       \n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "                    \n",
    "            # epochごとのlossと正解率\n",
    "            t_epoch_finish = time.time()\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double(\n",
    "            ) / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            a=float(\"{:.4f}\".format(epoch_acc))\n",
    "                \n",
    "            t_epoch_start = time.time()\n",
    "            \n",
    "            if count == 0:\n",
    "                Loss_train.append(epoch_loss)\n",
    "                Accuracy_train.append(a)\n",
    "            elif count==1:\n",
    "                Loss_val.append(epoch_loss)\n",
    "                Accuracy_val.append(a)\n",
    "        Epochs.append(epoch+1)\n",
    "    t=time.time()\n",
    "    time_val = t-start\n",
    "    print(\"Time:{:.4f}sec\".format(time_val))\n",
    "        \n",
    "    return net,Epochs,Loss_train,Accuracy_train,Loss_val,Accuracy_val,time_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 学習・検証を実行する。1epochに20分ほどかかります\n",
    "num_epochs = 20\n",
    "net_trained, Epochs, Loss_train, Accuracy_train, Loss_val, Accuracy_val, time_val = train_model(net, dataloaders_dict,\n",
    "                          criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習したネットワークパラメータを保存します\n",
    "save_path = 'bert_fine_tuning.pth'\n",
    "torch.save(net_trained.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#cc=nn.Softmax(dim=1)\n",
    "# テストデータでの正解率を求める\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_trained.eval()   # モデルを検証モードに\n",
    "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
    "\n",
    "# epochの正解数を記録する変数\n",
    "epoch_corrects = 0\n",
    "\n",
    "predicted_label=[]#予測ラベル\n",
    "ture_label=[]#正解ラベル\n",
    "\n",
    "score_0=[]#0のスコア\n",
    "score_1=[]#1のスコア\n",
    "count=0\n",
    "\n",
    "start =time.time()\n",
    "\n",
    "for batch in tqdm(test_dl):  # testデータのDataLoader\n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = batch.Text[0].to(device)  # 文章\n",
    "    labels = batch.Label.to(device)  # ラベル\n",
    "    epoch_loss=0.0\n",
    "\n",
    "    \n",
    "    # 順伝搬（forward）計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # 入力\n",
    "        outputs = net_trained(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n",
    "        \n",
    "        #outputs=cc(outputs)\n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                s1=outputs[i][0].item()\n",
    "                s2=outputs[i][1].item()\n",
    "                p_label = preds[i].item()\n",
    "                t_label = labels[i].item()\n",
    "                score_0.append(s1)\n",
    "                score_1.append(s2)\n",
    "                predicted_label.append(p_label)\n",
    "                ture_label.append(t_label)\n",
    "                count+=1\n",
    "            except:\n",
    "                break\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        \n",
    "epoch_loss = epoch_loss / len(test_dl.dataset)\n",
    "\n",
    "t=time.time()\n",
    "time_test = t-start\n",
    "print(\"Time:{:.4f}sec\".format(time_test))\n",
    "\n",
    "print('Loss:{:.4f}'.format(epoch_loss))\n",
    "# 正解率\n",
    "epoch_acc = epoch_corrects.double() / len(test_dl.dataset)\n",
    "\n",
    "print('テストデータ{}個での正解率：{:.4f}'.format(len(test_dl.dataset), epoch_acc))\n",
    "\n",
    "\n",
    "#df = pd.read_csv(\"./data/test.csv\", engine=\"python\", encoding=\"sjis\")\n",
    "#df = pd.read_csv(\"./data/test.csv\", engine=\"python\", encoding=\"utf-8-sig\")\n",
    "df = pd.read_csv(\"./data/test.csv\", names=(\"TEXT\", \"LABEL\"), engine=\"python\", encoding=\"utf-8-sig\")\n",
    "#df[\"TEXT\"] = np.nan   #予測列を追加\n",
    "#df[\"LABEL\"] = np.nan   #予測列を追加\n",
    "df[\"PREDICT\"] = np.nan   #予測列を追加\n",
    "df[\"AUC+\"] = np.nan   #予測列を追加\n",
    "df[\"AUC-\"] = np.nan   #予測列を追加\n",
    "\n",
    "for index in range(count):\n",
    "    df.at[index, \"PREDICT\"] = predicted_label[index]\n",
    "    \n",
    "    df.at[index, \"AUC+\"] = score_0[index]\n",
    "    df.at[index, \"AUC-\"] = score_1[index]\n",
    "    \n",
    "    \n",
    "df.to_csv(path_result+\"predicted_test.csv\", encoding=\"utf-8-sig\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊検証時のROC(AUC)の出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "fpr, tpr, thresholds = roc_curve(ture_label, score_1)\n",
    "plt.axes().set_aspect(\"equal\")\n",
    "#plt.plot(fpr, tpr,marker=\".\")\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.grid()\n",
    "plt.savefig(path_result+\"roc_curve.png\")\n",
    "auc=roc_auc_score(ture_label,score_1)\n",
    "print(\"AUC:{}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "precision, recall, thresholds = precision_recall_curve(ture_label, score_1)\n",
    "#plt.plot(fpr, tpr,marker=\".\")\n",
    "plt.axes().set_aspect(\"equal\")\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.grid()\n",
    "plt.savefig(path_result+\"precision_recall.png\")\n",
    "pr_auc=auc(recall, precision)\n",
    "print(\"AUC:{}\".format(pr_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊混同行列と精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#混合行列の表示（評価）\n",
    "y_true =[]\n",
    "y_pred =[]\n",
    "df = pd.read_csv(path_result+\"predicted_test.csv\", engine=\"python\", encoding=\"utf-8-sig\")\n",
    "#df = pd.read_csv(\"./result/predicted_test.csv\", engine=\"python\", encoding=\"sjis\")\n",
    "for index, row in df.iterrows():\n",
    "    if row['LABEL'] == 0:\n",
    "        y_true.append(\"負例\")\n",
    "    if row['LABEL'] ==1:\n",
    "        y_true.append(\"正例\")\n",
    "    if row['PREDICT'] ==0:\n",
    "        y_pred.append(\"負例\")\n",
    "    if row['PREDICT'] ==1:\n",
    "        y_pred.append(\"正例\")\n",
    "\n",
    "    \n",
    "print(len(y_true))\n",
    "print(len(y_pred))\n",
    "\n",
    "\n",
    "# 混同行列(confusion matrix)の取得\n",
    "labels = [\"負例\", \"正例\"]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "# データフレームに変換\n",
    "cm_labeled = pd.DataFrame(cm, columns=labels, index=labels)\n",
    "\n",
    "# 結果の表示\n",
    "cm_labeled.to_csv(path_result+\"confusion_matrix.csv\", encoding=\"utf-8-sig\")\n",
    "cm_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true =[]\n",
    "y_pred =[]\n",
    "df = pd.read_csv(path_result+\"predicted_test.csv\", engine=\"python\", encoding=\"utf-8-sig\")\n",
    "#df = pd.read_csv(\"./result/predicted_test.csv\", engine=\"python\", encoding=\"sjis\")\n",
    "for index, row in df.iterrows():\n",
    "    y_true.append(row[\"LABEL\"])\n",
    "    y_pred.append(row[\"PREDICT\"])\n",
    "\"\"\"       \n",
    "print(\"正解率（すべてのサンプルのうち正解したサンプルの割合）={}%\".format((round(accuracy_score(y_true, y_pred),2)) *100 ))\n",
    "print(\"適合率（positiveと予測された中で実際にpositiveだった確率）={}%\".format((round(precision_score(y_true, y_pred),2)) *100 ))\n",
    "print(\"再現率（positiveなデータに対してpositiveと予測された確率）={}%\".format((round(recall_score(y_true, y_pred),2)) *100 ))\n",
    "print(\"F1（適合率と再現率の調和平均）={}%\".format((round(f1_score(y_true, y_pred),2)) *100 ))\n",
    "\"\"\"\n",
    "print(\"正解率（すべてのサンプルのうち正解したサンプルの割合）={}\".format((accuracy_score(y_true, y_pred))))\n",
    "print(\"適合率（positiveと予測された中で実際にpositiveだった確率）={}\".format((precision_score(y_true, y_pred))))\n",
    "print(\"再現率（positiveなデータに対してpositiveと予測された確率）={}\".format((recall_score(y_true, y_pred))))\n",
    "print(\"F1（適合率と再現率の調和平均）={}\".format((f1_score(y_true, y_pred))))\n",
    "with open(\"{}auc_f.txt\".format(path_result),\"a\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"roc_curve, AUC:{}\\n\".format(auc))\n",
    "    f.write(\"precision_recall, AUC:{}\\n\".format(pr_auc))\n",
    "    f.write(\"正解率（すべてのサンプルのうち正解したサンプルの割合）={}\\n\".format((accuracy_score(y_true, y_pred))))\n",
    "    f.write(\"適合率（positiveと予測された中で実際にpositiveだった確率）={}\\n\".format((precision_score(y_true, y_pred))))\n",
    "    f.write(\"再現率（positiveなデータに対してpositiveと予測された確率）={}\\n\".format((recall_score(y_true, y_pred))))\n",
    "    f.write(\"F1（適合率と再現率の調和平均）={}\\n\".format((f1_score(y_true, y_pred))))\n",
    "    f.write(\"Time_val:{:.4f}sec\\n\".format(time_val))\n",
    "    f.write(\"Time_test:{:.4f}sec\\n\".format(time_test))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
