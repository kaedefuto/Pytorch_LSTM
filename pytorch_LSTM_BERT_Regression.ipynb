{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊PYTORCHによるLSTMの実装\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 環境:Ubuntu18.04\n",
    "- GPU:Quadro RTX 8000\n",
    "- ドライバー:NVIDIA-SMI 460.32.03, Driver Version: 460.32.03, CUDA Version: 11.2\n",
    "- ./data:train.tsv, test.tsv,test.csv\n",
    "- EarlyStoppingを利用する場合はhttps://github.com/Bjarten/early-stopping-pytorch からpytorchtools.pyをutilsにインストールし学習・検証のコメントアウトを外すこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊事前準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n pytorch_bert python=3.7\n",
    "!conda activate pytorch_bert\n",
    "!conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge\n",
    "!conda install jupyter\n",
    "!pip install transformers==4.5.0\n",
    "!pip install torchtext==0.9.0\n",
    "!pip install tqdm\n",
    "!pip install attrdict\n",
    "!pip install mojimoji\n",
    "!pip install mecab-python3\n",
    "!pip install mecabrc\n",
    "!pip install unidic_lite\n",
    "!pip install sentencepiece\n",
    "!pip install fugashi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊ライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import glob\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import mojimoji\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer, BertModel, AutoTokenizer\n",
    "path_result=\"./result/\"\n",
    "path_weights=\"./weights/\"\n",
    "max_length=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):\n",
    "    # 半角・全角の統一\n",
    "    text = mojimoji.han_to_zen(text) \n",
    "    # 改行、半角スペース、全角スペースを削除\n",
    "    text = re.sub('\\r', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('　', '', text)\n",
    "    text = re.sub(' ', '', text)\n",
    "    #どっちでも\n",
    "    text = re.sub(',', '', text)\n",
    "\n",
    "    # 数字文字の一律「0」化\n",
    "    text = re.sub(r'[0-9 ０-９]+', '0', text)  # 数字\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        #if (p == \".\"):\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊TorchtextでDatasetの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数のシードを設定\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "# 単語分割用のTokenizerを用意\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "#tokenizer_bert = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "\n",
    "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer.encode(text, max_length=max_length, truncation=True, return_tensors='pt')[0]\n",
    "    return ret\n",
    "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=False, lower=False, include_lengths=True, batch_first=True, fix_length=max_length, pad_token=0)\n",
    "\n",
    "LABEL = torchtext.legacy.data.Field(sequential=False, use_vocab=False, dtype = torch.float)\n",
    "\n",
    "# フォルダ「data」から各tsvファイルを読み込みます\n",
    "train_val_ds, test_ds = torchtext.legacy.data.TabularDataset.splits(\n",
    "    path=\"data/\", train='train.tsv',\n",
    "    test='test.tsv', format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊Dataloaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # BERTでは16、32あたりを使用する\n",
    "# torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
    "train_ds, val_ds = train_val_ds.split(split_ratio=0.8, random_state=random.seed(1234))\n",
    "\n",
    "train_dl = torchtext.legacy.data.Iterator(train_val_ds, batch_size=batch_size, train=True)\n",
    "train_dl_val = torchtext.legacy.data.Iterator(train_ds, batch_size=batch_size, train=True)\n",
    "val_dl = torchtext.legacy.data.Iterator(val_ds, batch_size=batch_size, train=False, sort=False)\n",
    "test_dl = torchtext.legacy.data.Iterator(test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict_val = {\"train\": train_dl_val, \"val\": val_dl}\n",
    "dataloaders_dict = {\"train\": train_dl}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認 テストデータのデータセットで確認\n",
    "batch = next(iter(test_dl))\n",
    "print(batch)\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチの1文目を確認してみる\n",
    "text_minibatch_1 = (batch.Text[0][1]).numpy()\n",
    "\n",
    "# IDを単語に戻す\n",
    "text = tokenizer_bert.convert_ids_to_tokens(text_minibatch_1)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "#tokenizer_bert=BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "text=tokenizer_bert(\"鬼滅の刃は面白いです\")\n",
    "print(text)\n",
    "text2=tokenizer_bert.convert_ids_to_tokens(text[\"input_ids\"])\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊LSTMモデルを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "#model_name=\"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "model_name=\"cl-tohoku/bert-base-japanese-v2\"\n",
    "net_bert=BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, net_bert, input_size, hidden_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        \n",
    "        # BERTモジュール\n",
    "        self.bert = net_bert  # BERTモデル\n",
    "        \n",
    "        # 前方向と後ろ方向の最後の隠れ層ベクトルを結合したものを受け取るので、hidden_dimを2倍している\n",
    "        self.bilstm = nn.LSTM(input_size=input_size,  hidden_size=hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.cls = nn.Linear(in_features=hidden_size * 2, out_features=1)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        with torch.no_grad():  # 勾配計算なし\n",
    "        # 単語ベクトルを計算\n",
    "            # 最終層の隠れ状態ベクトルを取得\n",
    "            #last_hidden_states = output[0]\n",
    "            output  = self.bert(input_ids)\n",
    "            encoded_layers = output['last_hidden_state']\n",
    "            pooler_output = output['pooler_output'] \n",
    "            \n",
    "        _, output_hc =self.bilstm(encoded_layers)\n",
    "        output = torch.cat([output_hc[0][0], output_hc[0][1]], dim=1)\n",
    "        \n",
    "        out = self.cls(output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル構築\n",
    "net = LSTMClassifier(net_bert, 768, 768)\n",
    "# 訓練モードに設定\n",
    "net.train()\n",
    "\n",
    "print('ネットワーク設定完了')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊LSTMのファインチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パターン1：全てのパラメータを更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "\n",
    "optimizer = optim.Adam(net.bilstm.parameters(), lr=1e-3)\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パターン2：BILSTMと識別器のパラメータを更新（BERTは更新しない）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
    "\n",
    "# 1. まず全部を、勾配計算Falseにしてしまう\n",
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. 最後のBILSTMを勾配計算ありに変更\n",
    "for name, param in net.bilstm.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. 識別器を勾配計算ありに変更\n",
    "for name, param in net.cls.named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 最適化手法の設定\n",
    "\n",
    "# BERTの元の部分はファインチューニング\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.cls.parameters(), 'lr': 1e-3}\n",
    "], betas=(0.9, 0.999))\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊学習・検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊ 開発データでハイパーパラメータを決定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "#from utils.pytorchtools import EarlyStopping\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    #エポック数,Acuraccy,Loss保存用\n",
    "    Epochs=[]\n",
    "    Loss_train=[]\n",
    "    Loss_val=[]\n",
    "    \n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "    #early_stopping = EarlyStopping(patience=10)\n",
    "    \n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "    n=0\n",
    "    \n",
    "    #時間\n",
    "    start =time.time()\n",
    "    \n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        \"\"\"\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early Stopping\")\n",
    "            break # 打ち切り\n",
    "        # epochごとの訓練と検証のループ\n",
    "        \"\"\"\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            count=0\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "                count=1\n",
    "            \n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # 開始時刻を保存\n",
    "            t_epoch_start = time.time()\n",
    "            t_iter_start = time.time()\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.Text[0].to(device)  # 文章\n",
    "                labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # Bertに入力\n",
    "                    outputs = net(inputs)\n",
    "\n",
    "                    #loss = torch.sqrt(criterion(outputs, labels)) # 損失を計算\n",
    "                    #loss = criterion(outputs, labels)\n",
    "                    loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "                    \n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        \n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            \n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec. || 本イテレーションのRMSE:{}'.format(iteration, loss.item(), duration, loss.item()))\n",
    "                            t_iter_start = time.time()\n",
    "                            \n",
    "                            #early_stopping(loss.item(), net)\n",
    "                                       \n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    \n",
    "            # epochごとのloss\n",
    "            t_epoch_finish = time.time()\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f}'.format(epoch+1, num_epochs, phase, epoch_loss))\n",
    "            \n",
    "            t_epoch_start = time.time()\n",
    "            \n",
    "            if count == 0:\n",
    "                Loss_train.append(epoch_loss)\n",
    "            elif count==1:\n",
    "                Loss_val.append(epoch_loss)\n",
    "                \n",
    "        Epochs.append(epoch+1)\n",
    "    t=time.time()\n",
    "    print(\"Time:{:.4f}sec\".format(t-start))\n",
    "        \n",
    "    return net, Epochs, Loss_train, Loss_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 学習・検証を実行する。1epochに20分ほどかかります\n",
    "num_epochs = 10\n",
    "net_trained, Epochs, Loss_train, Loss_val= train_model(net, dataloaders_dict_val,\n",
    "                          criterion, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊学習時のEpochsごとのLossを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.axes().set_aspect(\"equal\")\n",
    "#Loss_test=[]\n",
    "plt.plot(Epochs, Loss_train,color=\"blue\",label=\"train\")\n",
    "plt.plot(Epochs, Loss_val,color=\"red\",label=\"val\")\n",
    "#plt.plot(Epochs,Loss_test,color=\"red\",label=\"test\")\n",
    "plt.xticks(Epochs)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig(path_result+\"Loss_val.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊ 全データで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "#from utils.pytorchtools import EarlyStopping\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    #エポック数,Acuraccy,Loss保存用\n",
    "    Epochs=[]\n",
    "    Loss_train=[]\n",
    "    Loss_val=[]\n",
    "    \n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "    #early_stopping = EarlyStopping(patience=10)\n",
    "    \n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "    n=0\n",
    "    \n",
    "    #時間\n",
    "    start =time.time()\n",
    "    \n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        \"\"\"\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early Stopping\")\n",
    "            break # 打ち切り\n",
    "        # epochごとの訓練と検証のループ\n",
    "        \"\"\"\n",
    "        \n",
    "        #for phase in ['train', 'val']:\n",
    "        for phase in ['train']:\n",
    "            count=0\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "                count=1\n",
    "            \n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # 開始時刻を保存\n",
    "            t_epoch_start = time.time()\n",
    "            t_iter_start = time.time()\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.Text[0].to(device)  # 文章\n",
    "                labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # Bertに入力\n",
    "                    outputs = net(inputs)\n",
    "\n",
    "                    #loss = torch.sqrt(criterion(outputs, labels)) # 損失を計算\n",
    "                    #loss = criterion(outputs, labels)\n",
    "                    loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "                    \n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        \n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec. || 本イテレーションのRMSE:{}'.format(iteration, loss.item(), duration, loss.item()))\n",
    "                            t_iter_start = time.time()\n",
    "                            \n",
    "                            #early_stopping(loss.item(), net)\n",
    "                                       \n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    \n",
    "            # epochごとのloss\n",
    "            t_epoch_finish = time.time()\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f}'.format(epoch+1, num_epochs, phase, epoch_loss))\n",
    "                \n",
    "            t_epoch_start = time.time()\n",
    "            \n",
    "            if count == 0:\n",
    "                Loss_train.append(epoch_loss)\n",
    "            elif count==1:\n",
    "                Loss_val.append(epoch_loss)\n",
    "        Epochs.append(epoch+1)\n",
    "        \n",
    "    t=time.time()\n",
    "    time_val = t-start\n",
    "    print(\"Time:{:.4f}sec\".format(time_val))\n",
    "        \n",
    "    return net, Epochs, Loss_train, Loss_val, time_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 学習・検証を実行する。1epochに20分ほどかかります\n",
    "num_epochs = 5\n",
    "net_trained, Epochs, Loss_train, Loss_val, time_val  = train_model(net, dataloaders_dict_val,\n",
    "                          criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習したネットワークパラメータを保存します\n",
    "save_path = path_weights+'bert_fine_tuning.pth'\n",
    "torch.save(net_trained.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊＊＊保存しているモデルを使用する場合＊＊＊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "#model_name=\"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "model_name=\"cl-tohoku/bert-base-japanese-v2\"\n",
    "net_bert=BertModel.from_pretrained(model_name)\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, net_bert, input_size, hidden_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        \n",
    "        # BERTモジュール\n",
    "        self.bert = net_bert  # BERTモデル\n",
    "        \n",
    "        # 前方向と後ろ方向の最後の隠れ層ベクトルを結合したものを受け取るので、hidden_dimを2倍している\n",
    "        self.bilstm = nn.LSTM(input_size=input_size,  hidden_size=hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.cls = nn.Linear(in_features=hidden_size * 2, out_features=1)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        with torch.no_grad():  # 勾配計算なし\n",
    "        # 単語ベクトルを計算\n",
    "            # 最終層の隠れ状態ベクトルを取得\n",
    "            last_hidden_states = outputs[0]\n",
    "            output  = self.bert(input_ids)\n",
    "            encoded_layers = output['last_hidden_state']\n",
    "            pooler_output = output['pooler_output'] \n",
    "            \n",
    "        _, output_hc =self.bilstm(encoded_layers)\n",
    "        output = torch.cat([output_hc[0][0], output_hc[0][1]], dim=1)\n",
    "        \n",
    "        out = self.cls(output)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル構築\n",
    "net_trained = BertClassifier(net_bert)\n",
    "\n",
    "# 訓練モードに設定\n",
    "net_trained.train()\n",
    "\n",
    "print('ネットワーク設定完了')\n",
    "\n",
    "#学習済みモデルを読み込む\n",
    "model_path = './weights/bert_fine_tuning.pth'\n",
    "net_trained.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊＊＊前処理用（テストデータのみ）＊＊＊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer, BertModel, AutoTokenizer\n",
    "path_result=\"./result/\"\n",
    "path_weights=\"./weights/\"\n",
    "max_length=128\n",
    "batch_size=16\n",
    "\n",
    "#テストデータのみ\n",
    "# 乱数のシードを設定\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "# 単語分割用のTokenizerを用意\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "#tokenizer_bert = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    # 半角・全角の統一\n",
    "    text = mojimoji.han_to_zen(text) \n",
    "    # 改行、半角スペース、全角スペースを削除\n",
    "    text = re.sub('\\r', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('　', '', text)\n",
    "    text = re.sub(' ', '', text)\n",
    "\n",
    "    # 数字文字の一律「0」化\n",
    "    text = re.sub(r'[0-9 ０-９]+', '0', text)  # 数字\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        #if (p == \".\"):\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "        return text\n",
    "\n",
    "# 前処理と単語分割をまとめた関数を定義\n",
    "# 単語分割の関数を渡すので、tokenizer_bertではなく、tokenizer_bert.tokenizeを渡す点に注意\n",
    "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert.tokenize):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer(text)  # tokenizer_bert\n",
    "    return ret\n",
    "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "#max_length = 256\n",
    "#max_length=128\n",
    "TEXT = torchtext.data.legacy.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True, lower=False, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"[CLS]\", eos_token=\"[SEP]\", pad_token=\"[PAD]\",unk_token='[UNK]')\n",
    "\n",
    "LABEL = torchtext.data.legacy.Field(sequential=False, use_vocab=False, dtype = torch.float)\n",
    "\n",
    "# フォルダ「data」から各tsvファイルを読み込みます\n",
    "# BERT用で処理するので、10分弱時間がかかります\n",
    "train_ds, test_ds = torchtext.legacy.data.TabularDataset.splits(\n",
    "    path=DATA_PATH, train='train_dumy.tsv',\n",
    "    test='test.tsv', format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "\n",
    "test_dl = torchtext.legacy.data.Iterator(test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {\"test\": test_dl}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#cc=nn.Softmax(dim=1)\n",
    "# テストデータでの正解率を求める\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_trained.eval()   # モデルを検証モードに\n",
    "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
    "\n",
    "predicted_label=[]#予測ラベル\n",
    "ture_label = []\n",
    "count=0\n",
    "\n",
    "start =time.time()\n",
    "for batch in tqdm(test_dl):  # testデータのDataLoader\n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = batch.Text[0].to(device)  # 文章\n",
    "    labels = batch.Label.to(device)  # ラベル\n",
    "    epoch_loss=0.0\n",
    "\n",
    "    \n",
    "    # 順伝搬（forward）計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        outputs = net_trained(inputs)\n",
    "\n",
    "        #loss = torch.sqrt(criterion(outputs, labels)) # 損失を計算\n",
    "        #loss = criterion(outputs, labels)\n",
    "        loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                p_label = outputs[i].item()\n",
    "                t_label = labels[i].item()\n",
    "                \n",
    "                predicted_label.append(p_label)\n",
    "                ture_label.append(t_label)\n",
    "                count+=1\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        \n",
    "epoch_loss = epoch_loss / len(test_dl.dataset)\n",
    "\n",
    "t=time.time()\n",
    "time_test =t-start\n",
    "print(\"Time:{:.4f}sec\".format(time_test))\n",
    "\n",
    "print('テストデータ{}個でのRMSE：{}'.format(len(test_dl.dataset), epoch_loss))\n",
    "\n",
    "df = pd.read_csv(\"./data/test.csv\", names=(\"TEXT\", \"LABEL\"), engine=\"python\", encoding=\"utf-8-sig\")\n",
    "df[\"PREDICT\"] = np.nan   #予測列を追加\n",
    "\n",
    "for index in range(count):\n",
    "    df.at[index, \"PREDICT\"] = predicted_label[index]\n",
    "    \n",
    "df.to_csv(\"./result/predicted_test.csv\", encoding=\"utf-8-sig\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊MSE・RMSEの出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse_loss = mean_squared_error(ture_label, predicted_label, squared=True)\n",
    "print(\"MSE：{}\".format(mse_loss))\n",
    "\n",
    "#import numpy as np\n",
    "#rmse = np.sqrt(mean_squared_error(ture_label, predicted_label, squared=True))\n",
    "rmse = mean_squared_error(ture_label, predicted_label, squared=False)\n",
    "print(\"RMSE：{}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊MAEの出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(ture_label, predicted_label)\n",
    "print(\"MAE{}\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊R2（決定係数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(ture_label, predicted_label)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# yyplot 作成関数\n",
    "def yyplot(y_label, y_pred):\n",
    "    yvalues = np.concatenate([y_label, y_pred])\n",
    "    ymin, ymax, yrange = np.amin(yvalues), np.amax(yvalues), np.ptp(yvalues)\n",
    "    plt.axes().set_aspect(\"equal\")\n",
    "    #plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_label, y_pred)\n",
    "    plt.plot([ymin - yrange * 0.01, ymax + yrange * 0.01], [ymin - yrange * 0.01, ymax + yrange * 0.01])\n",
    "    plt.xlim(ymin - yrange * 0.01, ymax + yrange * 0.01)\n",
    "    plt.ylim(ymin - yrange * 0.01, ymax + yrange * 0.01)\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Pred')\n",
    "    plt.savefig(path_result+\"pred.png\")\n",
    "    #plt.title('Observed-Predicted Plot')\n",
    "    #plt.tick_params(labelsize=1)\n",
    "    plt.show()\n",
    "    return plt\n",
    "\n",
    "# yyplot の実行例\n",
    "np.random.seed(0)\n",
    "y_obs = np.random.normal(size=(1000, 1))\n",
    "y_pred = y_obs + np.random.normal(scale=0.3, size=(1000, 1))\n",
    "fig = yyplot(ture_label, predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{}RMSE.txt\".format(path_result),\"a\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"MSE:{}\\n\".format(mse_loss))\n",
    "    f.write(\"RMSE:{}\\n\".format(rmse))\n",
    "    f.write(\"MAE:{}\\n\".format(mae))\n",
    "    f.write(\"R2:{}\\n\".format(r2))\n",
    "    f.write(\"Time_val:{:.4f}sec\\n\".format(time_val))\n",
    "    f.write(\"Time_test:{:.4f}sec\\n\".format(time_test))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
